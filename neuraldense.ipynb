{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class Network():\n",
        "    def __init__(self, nnodes, ninputs, lamda=0):\n",
        "        self.lamda = lamda\n",
        "        self.weight = np.random.randn(ninputs, nnodes) * np.sqrt(2. / ninputs) #xaiver initialization\n",
        "        self.bias = np.random.rand(nnodes) * 0.01\n",
        "        self.sdw = np.zeros((ninputs, nnodes))\n",
        "        self.sdb = np.zeros(nnodes)\n",
        "        self.vdw = np.zeros((ninputs, nnodes))\n",
        "        self.vdb = np.zeros(nnodes)\n",
        "        self.t = 0\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.dot(inputs, self.weight) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        self.gradient_weight = np.dot(self.input.T, gradient)\n",
        "        self.gradient_bias = np.sum(gradient, axis=0)\n",
        "        self.gradient_input = np.dot(gradient, self.weight.T)\n",
        "        return self.gradient_input\n",
        "\n",
        "    def calculate(self, optimizer):\n",
        "        if optimizer == 'adam':\n",
        "            self.t += 1\n",
        "            beta1, beta2 = 0.9, 0.999\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            self.sdw = beta2 * self.sdw + (1 - beta2) * (self.gradient_weight ** 2)\n",
        "            self.sdb = beta2 * self.sdb + (1 - beta2) * (self.gradient_bias ** 2)\n",
        "\n",
        "            self.vdw = beta1 * self.vdw + (1 - beta1) * self.gradient_weight\n",
        "            self.vdb = beta1 * self.vdb + (1 - beta1) * self.gradient_bias\n",
        "\n",
        "            # Bias correction for adam optimizer for the starting difference while using exponantially weighted average\n",
        "            sdw_corrected = self.sdw / (1 - beta2 ** self.t)\n",
        "            sdb_corrected = self.sdb / (1 - beta2 ** self.t)\n",
        "            vdw_corrected = self.vdw / (1 - beta1 ** self.t)\n",
        "            vdb_corrected = self.vdb / (1 - beta1 ** self.t)\n",
        "\n",
        "            self.sdw_corrected = sdw_corrected\n",
        "            self.sdb_corrected = sdb_corrected\n",
        "            self.vdw_corrected = vdw_corrected\n",
        "            self.vdb_corrected = vdb_corrected\n",
        "\n",
        "    def update(self, learning_rate, optimizer):\n",
        "        if optimizer == 'adam':\n",
        "            self.weight -= learning_rate * self.vdw_corrected / (np.sqrt(self.sdw_corrected) + 1e-8)\n",
        "            self.bias -= learning_rate * self.vdb_corrected / (np.sqrt(self.sdb_corrected) + 1e-8)\n",
        "        else:\n",
        "            self.weight -= learning_rate * self.gradient_weight\n",
        "            self.bias -= learning_rate * self.gradient_bias\n",
        "\n",
        "    def l2(self):\n",
        "        return np.sum(self.weight ** 2)\n",
        "\n",
        "class Relu():\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradients):\n",
        "        self.gradient = gradients * (self.input > 0) #why not self.output>>>because we need a boolean return\n",
        "        return self.gradient\n",
        "\n",
        "class Sigmoid():\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "        return self.dinputs\n",
        "\n",
        "class Softmax():\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        return gradient\n",
        "\n",
        "class CategoricalCrossEntropyLoss():\n",
        "    def forward(self, probs, true_outputs, layers):\n",
        "        clipped_probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
        "        loss_data = -np.sum(true_outputs * np.log(clipped_probs)) / (len(true_outputs) + 1e-8)\n",
        "\n",
        "        l2_terms = [layer.lamda * np.sum(layer.l2()) for layer in layers]\n",
        "        loss_weight = 0.5 * np.sum(l2_terms) / (len(true_outputs) +  1e-8)\n",
        "        return loss_data + loss_weight\n",
        "\n",
        "    def accuracy(self, probs, true_outputs):\n",
        "\n",
        "        prediction=np.argmax(probs, axis=1)\n",
        "        true_label=np.argmax(true_outputs, axis=1)\n",
        "        accuracy=np.mean(prediction == true_label)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, probs, true_outputs):\n",
        "        samples = len(true_outputs)\n",
        "\n",
        "        self.dinputs = (probs - true_outputs) / samples\n",
        "        return self.dinputs\n",
        "\n",
        "class BinaryCrossEntropyLoss():\n",
        "    def forward(self, y_pred, y_true, layers):\n",
        "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        loss_data = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        return loss_data\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "        dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "        self.dinputs = (dvalues - y_true) / len(y_true)\n",
        "        return self.dinputs\n",
        "\n",
        "class Fit():\n",
        "    def __init__(self, layers,epochs ,loss_function, layers_for_fit, learning_rate, optimizer='gradient'):\n",
        "        self.layers_for_fit = layers_for_fit\n",
        "        self.layers = layers\n",
        "        self.loss_function = loss_function\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs=epochs\n",
        "    def fit(self, X_train, y_train,X_test,y_test, batch_size):\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            epoch_loss_val = 0\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_inputs = X_train[i:i + batch_size]\n",
        "                batch_validate=X_test[i:i + batch_size]\n",
        "                batch_true_outputs = y_train[i:i + batch_size]\n",
        "                batch_validate_outputs = y_test[i:i + batch_size]\n",
        "\n",
        "                x = batch_inputs\n",
        "                for layer in self.layers_for_fit:\n",
        "                    x = layer.forward(x)\n",
        "\n",
        "                loss = self.loss_function.forward(x, batch_true_outputs, self.layers)\n",
        "                epoch_loss += loss  # Accumulate batch loss\n",
        "\n",
        "                gradient = self.loss_function.backward(x, batch_true_outputs)\n",
        "                for layer in reversed(self.layers_for_fit):\n",
        "                    gradient = layer.backward(gradient)\n",
        "\n",
        "                for layer in self.layers:\n",
        "                    layer.calculate(self.optimizer)\n",
        "\n",
        "                for layer in self.layers:\n",
        "                    layer.update(self.learning_rate, self.optimizer)\n",
        "\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(X_train) * batch_size}\")  # Print average loss for the epoch\n",
        "            epoch_accuracy = 0\n",
        "            epoch_loss_val = 0\n",
        "            for i in range(0,len(X_test),batch_size):\n",
        "                batch_validate = X_test[i:i + batch_size]\n",
        "                batch_validate_true = y_test[i:i + batch_size]\n",
        "\n",
        "                x2=batch_validate\n",
        "                for layer in self.layers_for_fit:\n",
        "                    x2=layer.forward(x2)\n",
        "\n",
        "                loss_validate = self.loss_function.forward(x2, batch_validate_true, self.layers)\n",
        "                accurate=self.loss_function.accuracy(x2, batch_validate_true)\n",
        "                epoch_loss_val += loss_validate\n",
        "                epoch_accuracy+=accurate\n",
        "            print(f\"Epoch {epoch + 1}, val_Loss: {epoch_loss_val / len(X_test) * batch_size},val_accuracy:{epoch_accuracy/ len(X_test) * batch_size}\")  # Print average loss for the epoch\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gRFjlXsW78pZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load MNIST data\n",
        "data = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = data.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]\n",
        "# Define network\n",
        "layer1 = Network(128, 784, lamda=0.01)\n",
        "layer2 = Network(64, 128, lamda=0.01)\n",
        "layer3 = Network(10, 64, lamda=0)\n",
        "relu1 = Relu()\n",
        "relu2 = Relu()\n",
        "softmax = Softmax()\n",
        "loss_function = CategoricalCrossEntropyLoss()\n",
        "\n",
        "layers_for_fit = [layer1, relu1, layer2, relu2, layer3, softmax]\n",
        "layers = [layer1, layer2, layer3]\n",
        "\n",
        "# Train model\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "optimizer = 'adam'\n",
        "model = Fit(layers, 5,loss_function, layers_for_fit, learning_rate, optimizer)\n",
        "model.fit(X_train, y_train,X_test, y_test, batch_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUI7hKn98Th6",
        "outputId": "8ce9312a-768b-47f0-ed19-54725974787d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6327762747199365\n",
            "Epoch 1, val_Loss: 0.33753377576406185,val_accuracy:0.9251\n",
            "Epoch 2, Loss: 0.3103527863087198\n",
            "Epoch 2, val_Loss: 0.2755986077950051,val_accuracy:0.9421\n",
            "Epoch 3, Loss: 0.2603352757623868\n",
            "Epoch 3, val_Loss: 0.24368666568203756,val_accuracy:0.9512\n",
            "Epoch 4, Loss: 0.23122326944442514\n",
            "Epoch 4, val_Loss: 0.22309545098250638,val_accuracy:0.957\n",
            "Epoch 5, Loss: 0.21130484296992344\n",
            "Epoch 5, val_Loss: 0.20947531802239328,val_accuracy:0.9616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hA5NV_GkEHlB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}