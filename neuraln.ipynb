{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, nnodes, ninputs, lamda=0):\n",
    "        self.lamda = lamda\n",
    "        self.weight = np.random.randn(ninputs, nnodes) * np.sqrt(2. / ninputs)\n",
    "        self.bias = np.random.rand(nnodes) * 0.01\n",
    "        self.sdw = np.zeros((ninputs, nnodes))\n",
    "        self.sdb = np.zeros(nnodes)\n",
    "        self.vdw = np.zeros((ninputs, nnodes))\n",
    "        self.vdb = np.zeros(nnodes)\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.input = inputs\n",
    "        self.output = np.dot(inputs, self.weight) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        self.gradient_weight = np.dot(self.input.T, gradient)\n",
    "        self.gradient_bias = np.sum(gradient, axis=0)\n",
    "        self.gradient_input = np.dot(gradient, self.weight.T)\n",
    "        return self.gradient_input\n",
    "\n",
    "    def calculate(self, optimizer):\n",
    "        if optimizer == 'adam':\n",
    "            self.t += 1\n",
    "            beta1, beta2 = 0.9, 0.999\n",
    "            epsilon = 1e-8\n",
    "\n",
    "            self.sdw = beta2 * self.sdw + (1 - beta2) * (self.gradient_weight ** 2)\n",
    "            self.sdb = beta2 * self.sdb + (1 - beta2) * (self.gradient_bias ** 2)\n",
    "\n",
    "            self.vdw = beta1 * self.vdw + (1 - beta1) * self.gradient_weight\n",
    "            self.vdb = beta1 * self.vdb + (1 - beta1) * self.gradient_bias\n",
    "\n",
    "            # Bias correction\n",
    "            sdw_corrected = self.sdw / (1 - beta2 ** self.t)\n",
    "            sdb_corrected = self.sdb / (1 - beta2 ** self.t)\n",
    "            vdw_corrected = self.vdw / (1 - beta1 ** self.t)\n",
    "            vdb_corrected = self.vdb / (1 - beta1 ** self.t)\n",
    "\n",
    "            self.sdw_corrected = sdw_corrected\n",
    "            self.sdb_corrected = sdb_corrected\n",
    "            self.vdw_corrected = vdw_corrected\n",
    "            self.vdb_corrected = vdb_corrected\n",
    "\n",
    "    def update(self, learning_rate, optimizer):\n",
    "        if optimizer == 'adam':\n",
    "            self.weight -= learning_rate * self.vdw_corrected / (np.sqrt(self.sdw_corrected) + 1e-8)\n",
    "            self.bias -= learning_rate * self.vdb_corrected / (np.sqrt(self.sdb_corrected) + 1e-8)\n",
    "        else:\n",
    "            self.weight -= learning_rate * self.gradient_weight\n",
    "            self.bias -= learning_rate * self.gradient_bias\n",
    "\n",
    "    def l2(self):\n",
    "        return np.sum(self.weight ** 2)\n",
    "\n",
    "class Relu():\n",
    "    def forward(self, inputs):\n",
    "        self.input = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        self.gradient = gradients * (self.input > 0)\n",
    "        return self.gradient\n",
    "\n",
    "class Sigmoid():\n",
    "    def forward(self, inputs):\n",
    "        self.input = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        return self.dinputs\n",
    "\n",
    "class Softmax():\n",
    "    def forward(self, inputs):\n",
    "        self.input = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        return gradient\n",
    "\n",
    "class CategoricalCrossEntropyLoss():\n",
    "    def forward(self, probs, true_outputs, layers):\n",
    "        clipped_probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "        loss_data = -np.sum(true_outputs * np.log(clipped_probs)) / len(true_outputs)\n",
    "        \n",
    "        l2_terms = [layer.lamda * np.sum(layer.l2()) for layer in layers]\n",
    "        loss_weight = 0.5 * np.sum(l2_terms) / len(true_outputs)\n",
    "        return loss_data + loss_weight\n",
    "\n",
    "    def backward(self, probs, true_outputs):\n",
    "        samples = len(true_outputs)\n",
    "        self.dinputs = (probs - true_outputs) / samples\n",
    "        return self.dinputs\n",
    "\n",
    "class BinaryCrossEntropyLoss():\n",
    "    def forward(self, y_pred, y_true, layers):\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        loss_data = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss_data \n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = (dvalues - y_true) / len(y_true)\n",
    "        return self.dinputs\n",
    "\n",
    "class Fit():\n",
    "    def __init__(self, layers, loss_function, layers_for_fit, learning_rate, optimizer='gradient'):\n",
    "        self.layers_for_fit = layers_for_fit\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, epochs, X_train, y_train, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0  # Initialize epoch loss\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_inputs = X_train[i:i + batch_size]\n",
    "                batch_true_outputs = y_train[i:i + batch_size]\n",
    "\n",
    "                x = batch_inputs\n",
    "                for layer in self.layers_for_fit:\n",
    "                    x = layer.forward(x)\n",
    "\n",
    "                loss = self.loss_function.forward(x, batch_true_outputs, self.layers)\n",
    "                epoch_loss += loss  # Accumulate batch loss\n",
    "\n",
    "                gradient = self.loss_function.backward(x, batch_true_outputs)\n",
    "                for layer in reversed(self.layers_for_fit):\n",
    "                    gradient = layer.backward(gradient)\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    layer.calculate(self.optimizer)\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    layer.update(self.learning_rate, self.optimizer)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(X_train) * batch_size}\")  # Print average loss for the epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = data.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
    "\n",
    "y_train = np.eye(10)[y_train]\n",
    "\n",
    "# Define network\n",
    "layer1 = Network(128, 784, lamda=0.01)\n",
    "layer2 = Network(64, 128, lamda=0.01)\n",
    "layer3 = Network(10, 64, lamda=0)\n",
    "relu1 = Relu()\n",
    "relu2 = Relu()\n",
    "softmax = Softmax()\n",
    "loss_function = CategoricalCrossEntropyLoss()\n",
    "\n",
    "layers_for_fit = [layer1, relu1, layer2, relu2, layer3, softmax]\n",
    "layers = [layer1, layer2, layer3]\n",
    "\n",
    "# Train model\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "optimizer = 'adam'\n",
    "model = Fit(layers, loss_function, layers_for_fit, learning_rate, optimizer)\n",
    "model.fit(5, X_train, y_train, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
